# -*- coding: utf-8 -*-
"""Distilbert Finetuning and LIME Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14dxrnVPhFFJiEbyy4OhxsaY1j1reM7yu
"""

! pip install datasets transformers accelerate evaluate degender-pronoun

from huggingface_hub import notebook_login

notebook_login()

!apt install git-lfs

from google.colab import drive
drive.mount('/content/drive')

from datasets import Dataset
import pandas as pd

#dataset_path = "/Users/benjamyu/workspace/NLP-Letters/data/sentence_sets_trimmed.csv"
dataset_path = "/content/drive/MyDrive/sentence_sets_trimmed.csv"
df = pd.read_csv(dataset_path, encoding='unicode_escape')

from degender_pronoun import degenderizer

D = degenderizer()

df['s1_s2'] = df['s1_s2'].apply(lambda x: D.degender(x) if len(x) > 5 else x)
#D.degender(df['s1_s2'][0])

dataset = Dataset.from_pandas(df).rename_column("applicant_gender", "label").class_encode_column("label").train_test_split(test_size=0.2)

df['s1_s2'][0]

df['s1_s2'][0]

df['s1_s2'] = df['s1_s2'].str.replace(' mr ',' ')
df['s1_s2'] = df['s1_s2'].str.replace(' mrs ',' ')
df['s1_s2'] = df['s1_s2'].str.replace(' ms ', '')

df['s1_s2'][27]

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
import evaluate
from datasets import load_metric
from torch import nn
import torch
#metric = load_metric('glue', 'mrpc')
metric = evaluate.load('f1')

model_checkpoint = "distilbert-base-uncased"
batch_size = 16
task = "nlp-letters-s1-s2-degendered-class-weighted"
metric_name = "f1"
data_column = "s1_s2"
model_name = model_checkpoint.split("/")[-1]
num_labels = 2

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # compute custom loss (suppose one has 2 labels with different weights)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([8.0, 1.0], device=model.device))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

args = TrainingArguments(
    f"{model_name}-finetuned-{task}",
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=10,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model=metric_name,
    push_to_hub=True,
)

def preprocess_function(sample):
    return tokenizer(sample[data_column], truncation=True, padding=True)

accuracy = load_metric("accuracy")
precision = load_metric("precision")
recall = load_metric("recall")
f1 = load_metric("f1")


confusion_metric = evaluate.load("confusion_matrix")

# def compute_metrics(eval_pred):
#     logits, labels = eval_pred
#     predictions = np.argmax(logits, axis=-1)
#     return confusion_metric.compute(predictions=predictions, references=labels)

def compute_metrics(eval_pred):
    x, y = eval_pred
    preds = np.argmax(x, -1)
    print(confusion_metric.compute(predictions=preds, references=y))
    return metric.compute(predictions=preds, references=y, average="macro")

encoded_dataset = dataset.map(preprocess_function, batched=True)

# trainer = Trainer(
#     model,
#     args,
#     train_dataset=encoded_dataset["train"],
#     eval_dataset=encoded_dataset["test"],
#     tokenizer=tokenizer,
#     compute_metrics=compute_metrics
# )


trainer = CustomTrainer(
    model,
    args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.evaluate()

trainer.push_to_hub()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = np.array([[131,  61],
       [ 30, 435]])
cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=['female','male'])
disp.plot(values_format='.2%')
#disp.plot()

disp = ConfusionMatrixDisplay(confusion_matrix=np.array([[ 35.0/657.0, 152.0/657.0], [289.0/657.0, 181.0/657.0]]),
                              display_labels=['female','male'])
disp.plot(values_format='.2%')

dataset['train'].features

!pip install eli5

!pip install scipy

import scipy
import numpy as np
def monkeypath_itemfreq(sampler_indices):
   return zip(*np.unique(sampler_indices, return_counts=True))

scipy.stats.itemfreq=monkeypath_itemfreq

import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from eli5.lime import TextExplainer




model_checkpoint = "ben-yu/distilbert-base-uncased-finetuned-nlp-letters-s1-s2-degendered"
batch_size = 16
task = "nlp-letters"
metric_name = "accuracy"
data_column = "full_text"
model_name = model_checkpoint.split("/")[-1]
num_labels = 2

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels, low_cpu_mem_usage=True)
def model_adapter(texts):

    all_scores = []
    for i in range(0, len(texts), 64):
        batch = texts[i:i+64]

        # use bert encoder to tokenize text
        encoded_input = tokenizer(batch,
          return_tensors='pt',
          padding=True,
          truncation=True,
          max_length=model.config.max_position_embeddings-2)
        # run the model
        output = model(**encoded_input)
        # by default this model gives raw logits rather
        # than a nice smooth softmax so we apply it ourselves here
        scores = output[0].softmax(1).detach().numpy()
        all_scores.extend(scores)
    return np.array(all_scores)


class_names = ['female', 'male']
text = """
I am pleased to provide my unqualified support for FIRST_NAME LAST_NAME  FIRST_NAME strikes me as someone who  will achieve at a high level in a residency program  she is mature   has other non   medical experiences   teach for america     and has a demonstrated record of superior achievement  her board scores are competitive and she has numerous honors in our major clinical rotations  I have spent time in conversation with her regarding anesthesiology and I have worked with her in the operating room  in the operating room   I was particularly struck by her aptitude and quiet confidence  I hope you ask her about her experiences while a teacher  I find these supplemental life experiences additive to any program  I know that we will make every effort to recruit her to unc
"""

te = TextExplainer(n_samples=5000, random_state=42, position_dependent=True)
te.fit(D.degender(text), model_adapter)
te.explain_prediction(target_names=class_names)

te.explain_weights(target_names=class_names)

class_names = ['female', 'male']
text = """
this letter serves as the department of medicine letter for FIRST_NAME LAST_NAME  these letters are written and compiled by the clerkship director and associate clerkship director with help from our clerkship coordinator  they are written in accordance with the cdim   apdim guidelines for department of medicine letters  LAST_NAME waived his right to review this letter  the core internal medicine clerkship at unm is 8   weeks in duration and consists of 6 weeks spent in the inpatient setting at our university and veteran's affairs hospitals as well as 2 weeks split on palliative care and ambulatory medicine  students final grade is on the ogsf scale   outstanding   good   satisfactory   fail   with 50 % of the grade based on clinical performance   25 % on shelf board   15 % on quizzes   consisting of students writing a problem list   assessment statement and differential diagnosis based on a paper case     and 10 % on professionalism  LAST_NAME received a clinical score of outstanding   a shelf board score of satisfactory   and an overall core im clerkship score of good  LAST_NAME received these comments on his core im clerkship    great documentation with accurate and well thought out assessments  active in requesting new tasks to help team and patient care  î attending e FIRST_NAME performed well on the rotation   specific strengths included actively seeking opportunities for patient care   using strong communication strategies to engender trust with patients   asking for continuous feedback and seeking opportunities to educate team   opportunities infections in hiv      attending e it was a pleasure working with FIRST_NAME on wards  FIRST_NAME's a team player   gets along well with other staff and patients and family  FIRST_NAME helped with team work and was active member of the team  his presentations were detailed oriented   write ups were well organized and thoughtful  he took extra initiative in educating the team about various topics including atrial fibrillation management   relevance of mrsa nares   sleep medications and medication interactions  he stayed late on several occasions to finish his work and help the team  he commits to patient care and delivers  î   attending  great patient care   prioritizes tasks   gathers all pertinent information from patients  outstanding knowledge   great clinical correlation   identifies new problems independently  great patient ownership   even followed important aspects of patients that he saw before but not seeing that day  really great bedside manner  spontaneously researches on patients cases and educates the team with what he learns  FIRST_NAME has confidence in himself and enjoys the profession he is about to practice   he has a great attitude   that makes working with him to be fun  I personally learned from his great positive attitude and disposition  professional with patients and team  î resident e sees a lot of patients   takes initiative to see new patients   good communication skills with patients   works well with residents   open to feedback  î resident in summary   FIRST_NAME completed internal medicine as his first clerkship in his phase il experience and performed very well clinically  he showed a strong work ethic during his clerkship within internal medicine   being noted by both unmh and va attendings to be an active member of the team and stay late when needed  he will FIRST_NAME a strong resident physician """

te = TextExplainer(n_samples=5000, random_state=42, position_dependent=True)
te.fit(D.degender(text), model_adapter)
te.explain_prediction(target_names=class_names)

te.explain_weights(target_names=class_names)