{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc6abb67-572c-415a-8afe-8c1c62509187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"/Users/benjamyu/workspace/NLP-Letters/data/sentence_sets_trimmed.csv\"\n",
    "df = pd.read_csv(dataset_path, encoding='unicode_escape')\n",
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e35211c1-fc01-43e8-aafb-a8f09f96bfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['applicant_id', 'document', 'writer_gender', 'letter_type', 'applicant_gender', 'applicant_identify', 'applicant_identify_group', 'standardized_lor', 'us_canadian', 'usmle_1', 'usmle_2', 'applicants_writers', 'TEXT', 'full_text', 's1', 's2', 's1_s2', 's3', 'full_text_tokens'],\n",
       "        num_rows: 2628\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['applicant_id', 'document', 'writer_gender', 'letter_type', 'applicant_gender', 'applicant_identify', 'applicant_identify_group', 'standardized_lor', 'us_canadian', 'usmle_1', 'usmle_2', 'applicants_writers', 'TEXT', 'full_text', 's1', 's2', 's1_s2', 's3', 'full_text_tokens'],\n",
       "        num_rows: 657\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca998da-dfb7-4b49-8eee-ccc9a5013f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamyu/miniconda3/envs/nlp-letters/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7823c353169140b69a77eb1e58d0e7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb839dcd75684861b105ce5f78b70826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/657 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(sample[data_column], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m encoded_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(preprocess_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     34\u001b[0m     model,\n\u001b[1;32m     35\u001b[0m     args,\n\u001b[1;32m     36\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mencoded_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     37\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mencoded_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     38\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m---> 39\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[43mcompute_metrics\u001b[49m\n\u001b[1;32m     40\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "task = \"nlp-letters\"\n",
    "metric_name = \"accuracy\"\n",
    "data_column = \"full_text\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "num_labels = 2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "def preprocess_function(sample):\n",
    "    return tokenizer(sample[data_column], truncation=True)\n",
    "\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    #compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3cfd28-6151-467d-acbc-0459e8a9e7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
